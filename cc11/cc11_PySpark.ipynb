{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m9yXqV3LigUA"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32fjpkeS-nYP"
      },
      "source": [
        "# Lab 11 Spark\n",
        "\n",
        "Author: ISTD, SUTD\n",
        "\n",
        "Title: Lab 11, Spark part 1\n",
        "\n",
        "Date: March 5, 2025\n",
        "\n",
        "## Learning outcome\n",
        "\n",
        "\n",
        "By the end of this lesson, you are able to\n",
        "\n",
        "* Submit PySpark jobs to a Spark cluster\n",
        "* Paralelize data processing using PySpark\n",
        "\n",
        "\n",
        "You can either execute this lab directly on the aws cluster with HDFS file system, or you can install PySpark in Google Colab and load the files locally. The main difference in coding is that we do not load the context from the HDFS filesystem, but instead just load a local file. Other than than that, all PySpark commands are the same.\n",
        "\n",
        "To run this lab, you can make a copy of this notebook or `File -> Open in Playground Mode`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9yXqV3LigUA"
      },
      "source": [
        "## Installing PySpark in Google Colab\n",
        "\n",
        "To install PySpark in Google Collab, execute the below cell. This will download Spark and install all necessary libraries for this lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxv7w_2y2bb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33a1793e-bdf3-4df2-c7e5-74a3ef052266"
      },
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Connecting to security.ub\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.82)] [Connecting to security.ub\u001b[0m\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [802 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [61.2 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,107 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,641 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,358 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,920 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,081 kB]\n",
            "Fetched 9,205 kB in 3s (3,394 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "45 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=4b2a2dfb0313aad603e0234fac657f092b23b50a72a696819512a794b4c14fed\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wordcount Example\n",
        "\n",
        "Let us first download the necessary data file. We can find it at `https://raw.githubusercontent.com/istd50043-2023-spring/cohort_problems/main/cc11/ex1/data.csv`.\n",
        "\n",
        "Colab lets us execute unix commands, as long as we prepend them with `!`. So let's download the file and move it into a new folder called `input`. While we are at it, let's create a folder called `output` as well."
      ],
      "metadata": {
        "id": "r_DCGGK3-E6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc10/data/TheCompleteSherlockHolmes.txt\n",
        "!mkdir input\n",
        "!mv TheCompleteSherlockHolmes.txt input/\n",
        "!mkdir output"
      ],
      "metadata": {
        "id": "z0LtAmEBoSDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f09a49-0019-46b7-aeb4-a99c52ec40d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-05 03:41:04--  https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc10/data/TheCompleteSherlockHolmes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3705628 (3.5M) [text/plain]\n",
            "Saving to: ‘TheCompleteSherlockHolmes.txt’\n",
            "\n",
            "TheCompleteSherlock 100%[===================>]   3.53M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-04-05 03:41:05 (38.0 MB/s) - ‘TheCompleteSherlockHolmes.txt’ saved [3705628/3705628]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check that the data.csv file downloaded by uncollapsing the left panel and checking the folder contents.\n",
        "\n",
        "Now we are ready to write our PySpark code. The goal is to write a simple wordcounter:"
      ],
      "metadata": {
        "id": "7ANacU_Go4bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# sc.stop() # uncomment this during debugging to restart your context in case execution stopped mid-way this cell.\n",
        "\n",
        "conf = SparkConf().setAppName(\"Wordcount Application\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# note that we load the text file directly with a local path instead of providing an hdfs url\n",
        "input_file_name = 'input/TheCompleteSherlockHolmes.txt'\n",
        "text_file = sc.textFile(input_file_name)\n",
        "\n",
        "counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
        "             .map(lambda word: (word, 1)) \\\n",
        "             .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "output_folder = './output/wordcount'\n",
        "counts.saveAsTextFile(output_folder)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "pzse2gw82OFu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "Write a PySpark application which takes a (set of) Comma-seperated-value (CSV) file(s) with 2 columns and output a CSV file with first two columns same as the input file, and the third column contains the values obtained by splitting the first column using the second column as delimiter.\n",
        "\n",
        "The input file can be found here: `https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex1/data.csv`.\n",
        "\n",
        "For example, given input from a file:\n",
        "\n",
        "```\n",
        "50000.0#0#0#,#\n",
        "0@1000.0@,@\n",
        "1$,$\n",
        "1000.00^Test_string,^\n",
        "```\n",
        "\n",
        "\n",
        "the program should output\n",
        "\n",
        "```\n",
        "50000.0#0#0#,#,['50000.0', '0', '0']\n",
        "0@1000.0@,@,['0', '1000.0', '']\n",
        "1$,$,['1', '']\n",
        "1000.00^Test_string,^,['1000.00', 'Test_string']\n",
        "```\n",
        "\n",
        "and write it to a file.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZHg9lXG8bwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex1/data.csv\n",
        "!mv data.csv input/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdHM6bG6XuDl",
        "outputId": "805691ae-3c93-4536-f567-91cb3097ced2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-05 03:49:22--  https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex1/data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 54 [text/plain]\n",
            "Saving to: ‘data.csv’\n",
            "\n",
            "\rdata.csv              0%[                    ]       0  --.-KB/s               \rdata.csv            100%[===================>]      54  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-05 03:49:22 (2.96 MB/s) - ‘data.csv’ saved [54/54]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def for_each(record):\n",
        "  cols = record.split(\",\")\n",
        "\n",
        "  if len(cols) > 1:\n",
        "    delimitter = cols[1]\n",
        "    extra_col = str(cols[0].split(delimitter))\n",
        "    cols.append(extra_col)\n",
        "\n",
        "  return \",\".join(cols)"
      ],
      "metadata": {
        "id": "9o9lDt3YXlbu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ./output/exercise_1\n",
        "\n",
        "sc.stop() # uncomment this during debugging to restart your context in case execution stopped mid-way this cell.\n",
        "\n",
        "conf = SparkConf().setAppName(\"Exercise 1\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# note that we load the text file directly with a local path instead of providing an hdfs url\n",
        "input_file_name = 'input/data.csv'\n",
        "text_file = sc.textFile(input_file_name)\n",
        "\n",
        "splits = text_file.map(for_each)\n",
        "\n",
        "for line in splits.collect():\n",
        "  print(line)\n",
        "\n",
        "output_folder = './output/exercise_1'\n",
        "splits.saveAsTextFile(output_folder)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "F1aLxWrqRyAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "692c7f12-4382-41ab-9f59-a32a6def3f09"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000.0#0#0#,#,['50000.0', '0', '0', '']\n",
            "0@1000.0@,@,['0', '1000.0', '']\n",
            "1$,$,['1', '']\n",
            "1000.00^Test_string,^,['1000.00', 'Test_string']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "Write PySpark application which aggregates (counts) a (set of) CSV file(s) with 4 columns based on its third column, the destination IP.\n",
        "\n",
        "The input file can be found here: `https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex2/data.csv`\n",
        "\n",
        "Given input\n",
        "\n",
        "```\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2\n",
        "```\n",
        "the program should output\n",
        "\n",
        "```\n",
        " 10.0.0.3.5001,13\n",
        " 10.0.0.2.54880,7\n",
        "```"
      ],
      "metadata": {
        "id": "tIYQgNbA963l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex2/data.csv\n",
        "!mkdir input/exercise_2\n",
        "!mv data.csv input/exercise_2"
      ],
      "metadata": {
        "id": "T-coxW5U9690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "507fade8-fbca-4b9b-fc42-93b68cdb36ba"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-05 04:11:27--  https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc11/ex2/data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1000 [text/plain]\n",
            "Saving to: ‘data.csv’\n",
            "\n",
            "\rdata.csv              0%[                    ]       0  --.-KB/s               \rdata.csv            100%[===================>]    1000  --.-KB/s    in 0s      \n",
            "\n",
            "2024-04-05 04:11:28 (35.5 MB/s) - ‘data.csv’ saved [1000/1000]\n",
            "\n",
            "mkdir: cannot create directory ‘input/exercise_2’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split(record):\n",
        "  split = record.split(\", \")\n",
        "  destination_ip = split[2]\n",
        "  return destination_ip, 1\n",
        "\n",
        "def count(current_count, change):\n",
        "  return current_count + change\n",
        "\n",
        "def format(record):\n",
        "  destination_ip = str(record[0])\n",
        "  count = str(record[1])\n",
        "  return f'{destination_ip}, {count}'"
      ],
      "metadata": {
        "id": "IJjcA8abdVSZ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ./output/exercise_2\n",
        "\n",
        "sc.stop() # uncomment this during debugging to restart your context in case execution stopped mid-way this cell.\n",
        "\n",
        "conf = SparkConf().setAppName(\"Exercise 2\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# note that we load the text file directly with a local path instead of providing an hdfs url\n",
        "input_file_name = 'input/exercise_2/data.csv'\n",
        "text_file = sc.textFile(input_file_name)\n",
        "\n",
        "count = text_file.map(split) \\\n",
        "          .reduceByKey(count) \\\n",
        "          .map(format)\n",
        "\n",
        "for line in count.collect():\n",
        "  print(line)\n",
        "\n",
        "output_folder = './output/exercise_2'\n",
        "count.saveAsTextFile(output_folder)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziY_KBOFdA4e",
        "outputId": "78eca65f-0ea8-49e8-c574-2356b16f646e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove './output/exercise_2': No such file or directory\n",
            "10.0.0.3.5001, 13\n",
            "10.0.0.2.54880, 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "\n",
        "Given the same input as Exercise 2, write a PySpark application which outputs the following:\n",
        "\n",
        "```\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604900,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604899,10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "05:49:56.604908, 10.0.0.3.5001,10.0.0.2.54880, 2, 7\n",
        "```\n",
        "\n",
        "\n",
        "In the event the input is very huge with too many unique destination IP values, can your program scale?\n",
        "\n",
        "\n",
        "The questions were adopted from `https://jaceklaskowski.github.io/spark-workshop/exercises/`\n"
      ],
      "metadata": {
        "id": "0BIgiKCh97D0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split(record):\n",
        "  return record.split(\", \")\n",
        "\n",
        "def ip_format(record):\n",
        "  destination_ip = record[2]\n",
        "  return destination_ip, 1\n",
        "\n",
        "def count(current_count, change):\n",
        "  return current_count + change\n",
        "\n",
        "def format_splits(record):\n",
        "  destination_ip = record[2]\n",
        "  remaining_data = \", \".join(record)\n",
        "  return destination_ip, remaining_data\n",
        "\n",
        "def format_results(record):\n",
        "  data = record[1][0]\n",
        "  count = record[1][1]\n",
        "  return f'{data}, {count}'"
      ],
      "metadata": {
        "id": "PeYlYik3itbh"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r ./output/exercise_3\n",
        "\n",
        "sc.stop() # uncomment this during debugging to restart your context in case execution stopped mid-way this cell.\n",
        "\n",
        "conf = SparkConf().setAppName(\"Exercise 3\")\n",
        "sc = SparkContext(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "# note that we load the text file directly with a local path instead of providing an hdfs url\n",
        "input_file_name = 'input/exercise_2/data.csv'\n",
        "text_file = sc.textFile(input_file_name)\n",
        "\n",
        "splits = text_file.map(split)\n",
        "counts = splits.map(ip_format).reduceByKey(count)\n",
        "joined = splits.map(format_splits).join(counts)\n",
        "results = joined.map(format_results)\n",
        "\n",
        "for line in results.collect():\n",
        "  print(line)\n",
        "\n",
        "output_folder = './output/exercise_3'\n",
        "results.saveAsTextFile(output_folder)\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "id": "IkEQswDg97Iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a5c0074-1794-4809-fd3c-a0c77e7ba7f6"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove './output/exercise_3': No such file or directory\n",
            "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604900, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604899, 10.0.0.2.54880, 10.0.0.3.5001, 2, 13\n",
            "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2, 7\n",
            "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2, 7\n",
            "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2, 7\n",
            "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2, 7\n",
            "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2, 7\n",
            "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2, 7\n",
            "05:49:56.604908, 10.0.0.3.5001, 10.0.0.2.54880, 2, 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8MwegY7PkihV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
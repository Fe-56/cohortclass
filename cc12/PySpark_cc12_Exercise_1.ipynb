{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "ggpf1o8eh-tq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvNd0CM59LZX"
      },
      "source": [
        "# PySpark exercise 1\n",
        "\n",
        "Author: ISTD, SUTD\n",
        "\n",
        "Title: Lab 12 Spark Part 2\n",
        "\n",
        "Date: March 5, 2025\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing PySpark in Google Colab\n",
        "\n",
        "To install PySpark in Google Collab, execute the below cell. This will download Spark and install all necessary libraries for this lab."
      ],
      "metadata": {
        "id": "uRE9eRLxOVcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n"
      ],
      "metadata": {
        "id": "PLPrH0jDOVqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bx2HTIRS9LZa"
      },
      "source": [
        "\n",
        "\n",
        "# Exercise 1\n",
        "\n",
        "In this exercise we are tasked to perform some data transformation using PySpark.\n",
        "\n",
        "For parts marked with **[CODE CHANGE REQUIRED]** you need to modify or complete the code before execution.\n",
        "For parts without **[CODE CHANGE REQUIRED]** , you can just run the given code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS1GQkzI9LZb"
      },
      "source": [
        "## Task:\n",
        "\n",
        "Change the below input into the specified output.\n",
        "\n",
        "### Input\n",
        "\n",
        "\n",
        "\n",
        "The input data are stored in a text file `https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc12/data/ex1/input.txt` which is a list of 2D coordinates in the following format.\n",
        "\n",
        "```text\n",
        "<label> 0:<x-value> 1:<y-value>\n",
        "...\n",
        "<label> 0:<x-value> 1:<y-value>\n",
        "```\n",
        "\n",
        "### Output\n",
        "\n",
        "The expected output of the transformation are two seperate TSV outputs, `ones` and `zeros`. Both are in the following format\n",
        "\n",
        "```text\n",
        "<x-value> <y-value> ...\n",
        "<x-value> <y-value>\n",
        "```\n",
        "\n",
        "\n",
        "For example, given the input file as the following\n",
        "\n",
        "```text\n",
        "1 0:102 1:230\n",
        "0 0:123 1:56\n",
        "0 0:22  1:2\n",
        "1 0:74 1:102\n",
        "```\n",
        "The output files in `ones` would be\n",
        "\n",
        "```text\n",
        "102 230\n",
        "74 102\n",
        "```\n",
        "\n",
        "The output files in `zeros` would be\n",
        "\n",
        "```text\n",
        "123 56\n",
        "22 2\n",
        "```\n",
        "where the space in between the numbers are tabs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npVZG2Fa9LZb"
      },
      "source": [
        "Run the following bash cell to upload the input data to HDFS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sutd50043/cohortclass/main/cc12/data/ex1/input.txt\n",
        "!mkdir input\n",
        "!mkdir output\n",
        "!mv input.txt ./input/"
      ],
      "metadata": {
        "id": "YEAiHFHmgpKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZlVfVed9LZb"
      },
      "source": [
        "Complete the following PySpark script so that it performs the above-mentioned transformation.\n",
        "\n",
        "**[CODE CHANGE REQUIRED]**\n",
        "\n",
        "```text\n",
        "Let r be an RDD, r.map(f) applies f to all elements in r and return a new RDD.\n",
        "r.filter(p) fitlers out elements e from r that satisfying p(e).\n",
        "r.saveAsTextFile(\"hdfs://...\") will save an RDD into hdfs.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "wsKnmg4I9LZb"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "sparkSession = SparkSession.builder.appName(\"Transformation notebook\").getOrCreate()\n",
        "sc = sparkSession.sparkContext\n",
        "\n",
        "\n",
        "def join(tokenized):\n",
        "    x = (tokenized[1].split(\":\"))[1]\n",
        "    y = (tokenized[2].split(\":\"))[1]\n",
        "    return \"\\t\".join([x,y])\n",
        "\n",
        "sc.appName = \"ETL (Transform) Example\"\n",
        "\n",
        "input_file_name = 'input/input.txt'\n",
        "input = sc.textFile(input_file_name)\n",
        "\n",
        "tokenizeds = input.map(lambda line : line.split(\" \"))\n",
        "tokenizeds.persist()\n",
        "\n",
        "ones = tokenizeds.filter(lambda tokenized : tokenized[0] == \"1\").map(join)\n",
        "\n",
        "output_folder_ones = \"./output/ones\"\n",
        "ones.saveAsTextFile(output_folder_ones)\n",
        "\n",
        "\n",
        "zeros = None\n",
        "# TODO: fix me\n",
        "\n",
        "output_folder_zeros = \"./output/zeros\"\n",
        "zeros.saveAsTextFile(output_folder_zeros)\n",
        "\n",
        "sc.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample answer\n"
      ],
      "metadata": {
        "id": "ggpf1o8eh-tq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T5RAZRL9LZc"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "```python\n",
        "zeros = tokenizeds.filter(lambda tokenized : tokenized[0] == \"0\").map(join)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOPriegO9LZc"
      },
      "source": [
        "## Test case\n",
        "\n",
        "Run the below bash cell to check the results\n",
        "\n",
        "It should be something like the following\n",
        "\n",
        "```text\n",
        "20/11/12 18:51:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
        "124\t253\n",
        "145\t255\n",
        "5\t63\n",
        "1\t168\n",
        "121\t254\n",
        "166\t222\n",
        "178\t255\n",
        "7\t176\n",
        "68\t45\n",
        "...\n",
        "```\n",
        "\n",
        "and\n",
        "\n",
        "```text\n",
        "124\t253\n",
        "145\t255\n",
        "5\t63\n",
        "1\t168\n",
        "121\t254\n",
        "166\t222\n",
        "178\t255\n",
        "7\t176\n",
        "68\t45\n",
        "64\t191\n",
        "...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "kxjrKQXR9LZc"
      },
      "outputs": [],
      "source": [
        "!head ./output/ones/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "ELpBYsln9LZc"
      },
      "outputs": [],
      "source": [
        "!head ./output/ones/*"
      ]
    }
  ]
}
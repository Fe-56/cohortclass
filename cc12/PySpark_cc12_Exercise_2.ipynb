{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iSRqLG-On5_B",
        "OLWdSm1isKOX",
        "75t1XM_Xnxl8",
        "SElbMzCNnxl9",
        "oD_kXWbmnxl9",
        "FEXosDkLnxl-"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark - Exercise 2\n",
        "\n",
        "Author: ISTD, SUTD\n",
        "\n",
        "Title: Lab 12 Spark Part 2\n",
        "\n",
        "Date: March 5, 2025\n",
        "\n",
        "\n",
        "Note: this lab was adapted to used a smaller dataset so as to run on Google Colab. If you want to use the full dataset, you will need to run it on your own machine. So you can consider downloading it and running in Jupyter, or use a [local runtime](https://research.google.com/colaboratory/local-runtimes.html) with Colab."
      ],
      "metadata": {
        "id": "4PAj0v7-n7Qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing PySpark in Google Colab\n",
        "\n",
        "To install PySpark in Google Collab, execute the below cell. This will download Spark and install all necessary libraries for this lab."
      ],
      "metadata": {
        "id": "iSRqLG-On5_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n"
      ],
      "metadata": {
        "id": "V9iljPeqn_m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02WjukTXnxl3"
      },
      "source": [
        "# Exercise 2\n",
        "\n",
        "In this exercise we use PySpark to build a binary classifier to classify a given tweet is about KPOP or other topics using a supervised machine learning technique, SVM.\n",
        "\n",
        "For parts marked with **[CODE CHANGE REQUIRED]** you need to modify or complete the code before execution.\n",
        "For parts without **[CODE CHANGE REQUIRED]** , you can just run the given code.\n",
        "\n",
        "The task here is to build a classifier to differentiate the KPOP tweets or otherwise.\n",
        "\n",
        "For example, the following tweet message falls into the category of Korean Pop because it seems talking about someone from korea\n",
        "```text\n",
        "crazy cool jae s lee's pic of street singer reflected in raindrops tuesday on 2nd ave  \n",
        "```\n",
        "On the other hand, the following tweet is not revelant to KPOP.\n",
        "```text\n",
        "accident closes jae valley rd drivers advised to avoid area seek alternate routes\n",
        "```\n",
        "To achieve the goal, we need to develop a classifier, which is a supervised machine learning technique. In this example, we consider using Support Vector Machine (SVM) as the classifier algorithm. On the higher level, we need to \"train\" the model with some manually labelled data and perform some tests against the trained model. As part of the input requirement the SVM expects the input data to be represented as a label (either yes or no, 1 or 0) accompanied by the feature vector. The feature vector is a vector of values which uniquely differentiate one entry from another ideally. In the machine learning context, features have to be fixed by the programmers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uJSZqfenxl5"
      },
      "source": [
        "## Uploading the data\n",
        "\n",
        "The data consists of many small files, all zipped into one.\n",
        "\n",
        "We will be working with a subsection of the data label_data_small.zip. If you want more realistic results, use the original dataset label_data.zip, however, this will be significantly slower without using hdfs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "iR3-0HYOnxl5"
      },
      "outputs": [],
      "source": [
        "!mkdir input\n",
        "!wget https://github.com/sutd50043/cohortclass/raw/main/cc12/data/ex2/label_data_small.zip\n",
        "!unzip label_data_small.zip -d input\n",
        "!mkdir output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the data. It consists of 2 folders in label_data: Kpop and othertweet.\n",
        "\n",
        "Each folder contains many text files, each with a tweet inside."
      ],
      "metadata": {
        "id": "eUzQ4VxxqFJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l input/label_data_small/"
      ],
      "metadata": {
        "id": "MR7OoMEnqEsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the files are very short and look like this:"
      ],
      "metadata": {
        "id": "mh6_RXtpqUcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! cat input/label_data_small/Kpop/predictionkorean140.txt"
      ],
      "metadata": {
        "id": "Rh0JYPf3qUkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSvx5zqJnxl6"
      },
      "source": [
        "## Importing and Setup\n",
        "\n",
        "Let's import all the required libraries.\n",
        "\n",
        "We make use of `numpy` a python library for numeric computation, and sets. Sets needs to be installed first:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sets"
      ],
      "metadata": {
        "id": "zTc3u_TnrAga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "NMra6VR-nxl6"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sets, math\n",
        "import numpy # make sure numpy is installed on all datanodes using the command pip3 install numpy\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.mllib import *\n",
        "from pyspark.mllib.regression import LabeledPoint\n",
        "from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.mllib.classification import SVMWithSGD\n",
        "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
        "\n",
        "sparkSession = SparkSession.builder.appName(\"SVM notebook\").getOrCreate()\n",
        "sc = sparkSession.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTDcEnd-nxl7"
      },
      "source": [
        "## Loading the data\n",
        "We load the data from the local disk. The `.sample(False,0.1)` is to perform sampling on the input dataset. If you are to run it on a full cluster, feel free to remove the sampling\n",
        "\n",
        "The first argument is boolean flag is called `withReplacement`. When it is `True`, it allows the same element to appear more than once.\n",
        "The second argument is the fraction of elements in the sampled results. `0.1` means we expect 10% of the entire data set in the samples. You might set it to a lower ratio if it takes too long to run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "YyZEJ0I5nxl7"
      },
      "outputs": [],
      "source": [
        "def remove_punct(tweet):\n",
        "    return re.sub('[\\'\".,!#]','',tweet)\n",
        "\n",
        "posTXT = sc.textFile(\"input/label_data_small/Kpop/*.txt\").sample(False,0.1).map(remove_punct)\n",
        "negTXT = sc.textFile(\"input/label_data_small/othertweet/*.txt\").sample(False,0.1).map(remove_punct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t1BSVM7nxl7"
      },
      "source": [
        "\n",
        "\n",
        "## Exercise 2.1 Build a language model using TFIDF\n",
        "\n",
        "In Natural Language Processing, we often model a language using bags of words model. The idea is to represent text in terms of vectors.\n",
        "\n",
        "One of the simple and effective method is to use Term-Frequency Inversed Document Frequency.\n",
        "\n",
        "$$\n",
        "TFIDF(w) = TF(w) * log(NDoc/DF(w))\n",
        "$$\n",
        "\n",
        "where *NDoc* is the number of documents.\n",
        "\n",
        "\n",
        "*  TF is actually the word count. For instance, consider the following text data.\n",
        "```text\n",
        "apple smart phones made by apple\n",
        "android smart phones made by others\n",
        "```\n",
        "We assume that each line is a document, hence there are two documents here.\n",
        "\n",
        "* The term frequency is\n",
        "```text\n",
        "apple, 2\n",
        "android, 1\n",
        "phones, 2\n",
        "smart, 2\n",
        "made, 2\n",
        "by, 2\n",
        "others, 1\n",
        "```\n",
        "The term frequency is basically the word count, i.e. the number of occurances of a word across all document.\n",
        "\n",
        "* The document frequency is\n",
        "\n",
        "```text\n",
        "apple, 1\n",
        "android, 1\n",
        "phones, 2\n",
        "smart, 2\n",
        "made, 2\n",
        "by, 2\n",
        "others, 1\n",
        "```\n",
        "\n",
        "The document frequency is the number of documents a word is mentioned.\n",
        "\n",
        "\n",
        "* IDF is is the total number of documents/records divided by the total number of the documents/records containing the words. We apply logarithmic to the quotient. The IDF for the above example is\n",
        "```text\n",
        "apple, log(2/1)\n",
        "android, log(2/1)\n",
        "phones, log(2/2)\n",
        "smart, log(2/2)\n",
        "made, log(2/2)\n",
        "by, log(2/2)\n",
        "others, log(2/1)\n",
        "```\n",
        "that is\n",
        "```text\n",
        "apple, 0.693\n",
        "android, 0.693\n",
        "phones, 0\n",
        "smart, 0\n",
        "made, 0\n",
        "by, 0\n",
        "others, 0.693\n",
        "```\n",
        "\n",
        "* TF-IDF is obtained by multiplying the TF with the IDF.\n",
        "```text\n",
        "apple, 1.386\n",
        "android, 0.693\n",
        "phones, 0\n",
        "smart, 0\n",
        "made, 0\n",
        "by, 0\n",
        "others, 0.693\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47iyHE6Hnxl7"
      },
      "source": [
        "### Define `tf`\n",
        "\n",
        "**[CODE CHANGE REQUIRED]**\n",
        "\n",
        "Complete the following snippet to define `tf`.\n",
        "\n",
        "Hint:\n",
        "tf is the same as word count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "K85Tsl3vnxl7"
      },
      "outputs": [],
      "source": [
        "def tf(terms):\n",
        "    '''\n",
        "    input\n",
        "    terms :  a RDD of lists of terms (words)\n",
        "    output\n",
        "    a RDD of pairs i.e. (word, tf_score)\n",
        "    '''\n",
        "    # TODO\n",
        "\n",
        "    # return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ap0MYEnnxl8"
      },
      "source": [
        "### Test Case for `tf`\n",
        "\n",
        "Run the following cell, you should see\n",
        "\n",
        "```\n",
        "[('apple', 2), ('by', 2), ('android', 1), ('smart', 2), ('made', 2), ('phones', 2), ('others', 1)]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "BYUxd38Qnxl8"
      },
      "outputs": [],
      "source": [
        "def one_grams(s):\n",
        "    return s.split()\n",
        "\n",
        "\n",
        "test_terms = [one_grams(\"apple smart phones made by apple\"), one_grams(\"android smart phones made by others\")]\n",
        "test_tf = tf(sc.parallelize(test_terms))\n",
        "test_tf.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample answer\n"
      ],
      "metadata": {
        "id": "OLWdSm1isKOX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KkOqes1nxl7"
      },
      "source": [
        "```python\n",
        "def tf(terms):\n",
        "    '''\n",
        "    input\n",
        "    terms :  a RDD of lists of terms (words)\n",
        "    output\n",
        "    a RDD of pairs i.e. (word, tf_score)\n",
        "    '''\n",
        "    # ANSWER\n",
        "    return terms.flatMap(lambda seq: map(lambda w:(w,1), seq)).reduceByKey(lambda x,y:x + y)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgf54hD-nxl8"
      },
      "source": [
        "### Define `df`\n",
        "\n",
        "**[CODE CHANGE REQUIRED]**\n",
        "\n",
        "Complete the following snippet to define `df`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "WrGNzI44nxl8"
      },
      "outputs": [],
      "source": [
        "def df(terms):\n",
        "    '''\n",
        "    input\n",
        "    terms :  a RDD of lists of terms (words)\n",
        "    output\n",
        "    a RDD of pairs i.e. (word, df_score)\n",
        "    '''\n",
        "    # TODO\n",
        "\n",
        "    # return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Hint:"
      ],
      "metadata": {
        "id": "2KZ4xMe8sl2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "`df` differs from tf with a little bit. Instead of outputting (word,1) for every word in a tweet directly, we should remove the duplicating words (within the same tweet) first.\n",
        "```\n"
      ],
      "metadata": {
        "id": "WBUkQYOOsoyz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzO2WhNgnxl8"
      },
      "source": [
        "### Test Case for `df`\n",
        "\n",
        "Run the following cell, you will see\n",
        "\n",
        "```\n",
        "[('apple', 1), ('by', 2), ('android', 1), ('smart', 2), ('made', 2), ('phones', 2), ('others', 1)]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "kq7goGXynxl8"
      },
      "outputs": [],
      "source": [
        "test_terms = [one_grams(\"apple smart phones made by apple\"), one_grams(\"android smart phones made by others\")]\n",
        "test_df = df(sc.parallelize(test_terms))\n",
        "test_df.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75t1XM_Xnxl8"
      },
      "source": [
        "#### Sample answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "def df(terms):\n",
        "    '''\n",
        "    input\n",
        "    terms :  a RDD of lists of terms (words)\n",
        "    output\n",
        "    a RDD of pairs i.e. (word, df_score)\n",
        "    '''\n",
        "    # ANSWER\n",
        "    return terms.flatMap(lambda seq: list(set(map(lambda w:(w,1), seq)))).reduceByKey(lambda x,y:x + y)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "tSgJqPEvtCeQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfV4FS-Gnxl8"
      },
      "source": [
        "### Define `tfidf`\n",
        "\n",
        "**[CODE CHANGE REQUIRED]**\n",
        "\n",
        "Complete the following snippet to define `tfidf`\n",
        "\n",
        "\n",
        "```text\n",
        "Let r be an RDD. r.count() returns the size of r.\n",
        "Let r1, r2 be RDDs of key-value pairs. r1.join(r2) joins two RDDs by keys.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "ceL1sGcEnxl9"
      },
      "outputs": [],
      "source": [
        "def tfidf(terms):\n",
        "    '''\n",
        "    input\n",
        "    terms:  a RDD of lists of terms (words)\n",
        "    output\n",
        "    a RDD of pairs i.e. (words, tfidf_score) sorted by tfidf_score in descending order.\n",
        "    '''\n",
        "    # TODO\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDpEcb23nxl9"
      },
      "source": [
        "### Test case for `tfidf`\n",
        "\n",
        "Run the following cell you will see\n",
        "\n",
        "```\n",
        "[('apple', 1.3862943611198906), ('android', 0.6931471805599453), ('others', 0.6931471805599453), ('by', 0.0), ('smart', 0.0), ('made', 0.0), ('phones', 0.0)]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "cXYcSQb-nxl9"
      },
      "outputs": [],
      "source": [
        "test_terms = [one_grams(\"apple smart phones made by apple\"), one_grams(\"android smart phones made by others\")]\n",
        "test_tfidf = tfidf(sc.parallelize(test_terms))\n",
        "test_tfidf.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SElbMzCNnxl9"
      },
      "source": [
        "### Sample answer\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "def tfidf(terms):\n",
        "    '''\n",
        "    input\n",
        "    terms:  a RDD of lists of terms (words)\n",
        "    output\n",
        "    a RDD of pairs i.e. (words, tfidf_score) sorted by tfidf_score in descending order.\n",
        "    '''\n",
        "    # ANSWER\n",
        "    dCount = terms.count()\n",
        "    tfreq = tf(terms)\n",
        "    dfreq = df(terms)\n",
        "    return tfreq.join(dfreq).map(lambda p :(p[0], p[1][0] * math.log(dCount/p[1][1]))).sortBy( lambda p : - p[1])\n",
        "```"
      ],
      "metadata": {
        "id": "KsTWGpn0tuFq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM7EkpVxnxl9"
      },
      "source": [
        "## Exercise 2.2 Defining the Label points\n",
        "\n",
        "Recall that each label point is a decimal value (the label) with a vector.\n",
        "\n",
        "* For all positive tweets (KPop tweets) the label will be `1` and for all negative tweets we set `0` as the label.\n",
        "* For the vector parts, we build them using the tweet messages and the top 150 TFIDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "NHowSB3Snxl9"
      },
      "outputs": [],
      "source": [
        "# You don't need to modify this cell\n",
        "def buildTopTFIDF(tweets,tokenizer):\n",
        "    '''\n",
        "    input\n",
        "    tweets: an RDD of texts|\n",
        "    tokenizer: a function turns a string into list of tokens\n",
        "\n",
        "    output\n",
        "    a list containing top 150 tfidf terms\n",
        "    '''\n",
        "    terms = tweets.map(tokenizer)\n",
        "    print('done with tweet map')\n",
        "    return map(lambda p:p[0], tfidf(terms).take(150))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_YGjf6Fnxl9"
      },
      "source": [
        "### Tokenizer\n",
        "**[CODE CHANGE REQUIRED]**\n",
        "We've been using single word tokens for the test cases. However sometime using a multi-word tokenizer will help improving the performance by taking the neighboring word into account.\n",
        "Define a `two_grams` tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "gQbr5X3Enxl9"
      },
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "\n",
        "\n",
        "def two_grams(str):\n",
        "\n",
        "   return None # TODO: fixme\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px0cKojHnxl9"
      },
      "source": [
        "### Test Case for `two_grams`\n",
        "\n",
        "Run the following you should see\n",
        "\n",
        "```text\n",
        "['The virus', 'virus that', 'that causes', 'causes COVID-19', 'COVID-19 is', 'is mainly', 'mainly transmitted', 'transmitted through', 'through droplets']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "Bd62V7Utnxl9"
      },
      "outputs": [],
      "source": [
        "s = \"The virus that causes COVID-19 is mainly  transmitted through droplets\"\n",
        "list(two_grams(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wItsbk4Pnxl9"
      },
      "source": [
        "The following cells build the top 150 TFIDF from the data that we loaded, you don't need to change anything. It might take a long time to run. Hence, I recommend that you connect your Google Drive first, so the results are stored permanently and can be reused.\n",
        "\n",
        "Google Colab may be too slow as it offers only 2 CPUs on the free tier. It should take about 10min if you are working with the reduced small dataset.\n",
        "\n",
        "To speed up compute so you can use the full dataset, you can download this notebook and run it on Jupyter locally. Or you can connect your local runtime to Colab. You can do this (if you have jupyter installed), by running:\n",
        "\n",
        "`jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888    --NotebookApp.port_retries=0`\n",
        "\n",
        "And then in Google Colab, connect to a local runtime and enter your the url you got after running the jupyter command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_kXWbmnxl9"
      },
      "source": [
        "### Sample answer\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "def to_ngrams(str, n):\n",
        "    words = str.split()\n",
        "    tokens = [words] * (n-1) # replicate the list of words for n-1 times\n",
        "    dropped = map(lambda p: p[0][p[1]:], zip(tokens, range(1,n)))\n",
        "    return reduce(lambda acc,ts:map(lambda p : p[0] + \" \" + p[1], zip(acc,ts)), dropped, words)\n",
        "\n",
        "def two_grams(str):\n",
        "    return to_ngrams(str, 2)\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "Imj3WeMAuT8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the Top TFIDFs"
      ],
      "metadata": {
        "id": "-WKGGF6k26uB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "XNYjoQCmnxl-"
      },
      "outputs": [],
      "source": [
        "topTFIDF =  buildTopTFIDF(posTXT + negTXT,two_grams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "T5Vsh69wnxl-"
      },
      "outputs": [],
      "source": [
        "type(topTFIDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uncgFV0bnxl-"
      },
      "source": [
        "\n",
        "\n",
        "## Defining `computeLP`\n",
        "**[CODE CHANGE REQUIRED]**\n",
        "Complete the following snippet.\n",
        "\n",
        "Concretely speaking, the `computeLP` function takes a label `1.0` or `0.0`, a sequence of string i.e. the 2-grams or 3-grams, and a array of top-N TF-IDF.\n",
        "\n",
        "For each tf-idf term, let's say `t` is the i-th top-N TF-IDF term, if `t` is in the sequence of strings, we should put a `1.0` at the i-th position of the output vector, otherwise it should be `0.0`.\n",
        "\n",
        "Hint:\n",
        "\n",
        "```text\n",
        "Convert all the words in the input text into a set instead of a list.\n",
        "The output vector should be of the same dimension as topTerms (AKA top 150 TFIDF).\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "HNA8TRSUnxl-"
      },
      "outputs": [],
      "source": [
        "def computeLP(label,text,tokenizer,topTerms):\n",
        "    '''\n",
        "    input\n",
        "    label : label 1 or 0\n",
        "    text : the text (String type)\n",
        "    tokenizer : the tokenizer\n",
        "    topTerms: the top TFIDF terms\n",
        "\n",
        "    output: a label point.\n",
        "    '''\n",
        "\n",
        "    seqSet = set(tokenizer(text))\n",
        "    scores = [0.0] * 150 # TODO: fixme\n",
        "\n",
        "    return LabeledPoint(label, Vectors.dense(scores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sf6FvBGnxl-"
      },
      "source": [
        "### Test Case for `computeLP`\n",
        "\n",
        "Run the following cell, you should see\n",
        "\n",
        "```\n",
        "LabeledPoint(1.0, [0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "zqggaeBVnxl-"
      },
      "outputs": [],
      "source": [
        "computeLP(1.0, \"I love yoo jae suk\", two_grams, topTFIDF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEXosDkLnxl-"
      },
      "source": [
        "### Sample answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "def computeLP(label,text,tokenizer,topTerms):\n",
        "    '''\n",
        "    input\n",
        "    label : label 1 or 0\n",
        "    text : the text (String type)\n",
        "    tokenizer : the tokenizer\n",
        "    topTerms: the top TFIDF terms\n",
        "    \n",
        "    output:\n",
        "    a label point.\n",
        "    '''\n",
        "    seqSet = set(tokenizer(text))\n",
        "    # ANSWER\n",
        "    scores = map(lambda t: 1.0 if t in seqSet else 0.0, list(topTerms))\n",
        "    return LabeledPoint(label, Vectors.dense(scores))\n",
        "````"
      ],
      "metadata": {
        "id": "TGtVrv8HzwEr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuNHScRxnxl-"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Let's train our model. The code is written for you, you don't need to change anything\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "896ZT4sHnxl_"
      },
      "outputs": [],
      "source": [
        "posLP = posTXT.map( lambda twt: computeLP(1.0, twt, two_grams, topTFIDF) )\n",
        "negLP = negTXT.map( lambda twt: computeLP(0.0, twt, two_grams, topTFIDF) )\n",
        "\n",
        "data = negLP + posLP\n",
        "\n",
        "\n",
        "# Split data into training (60%) and test (40%).\n",
        "\n",
        "splits = data.randomSplit([0.6,0.4],seed = 11)\n",
        "training = splits[0].cache()\n",
        "test = splits[1]\n",
        "\n",
        "# Run training algorithm to build the model\n",
        "num_iteration = 100\n",
        "model = SVMWithSGD.train(training,num_iteration)\n",
        "\n",
        "# With the full dataset, this will takes about 20 mins on a 4-core intel i7 processor 3.8GHZ with hyperthreading.\n",
        "# We have loaded a very reduced dataset for demo purposes, so it will run in a matter of minutes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be1zBkA3nxl_"
      },
      "source": [
        "## Exercise 2.3 Evaluating the model\n",
        "\n",
        "We apply the trained model to our testing data and evaluate the performance of our model. It should be around 84% accurate if you use the full dataset, with the reduced one, the model stays random at 0.5.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "ArAOk2xznxl_"
      },
      "outputs": [],
      "source": [
        "model.clearThreshold()\n",
        "# Compute raw scores on the test set\n",
        "score_and_labels = test.map( lambda point: (float(model.predict(point.features)), point.label) )\n",
        "\n",
        "\n",
        "# Get the evaluation metrics\n",
        "metrics = BinaryClassificationMetrics(score_and_labels)\n",
        "au_roc = metrics.areaUnderROC\n",
        "\n",
        "print(\"Area under ROC = %s\" % str(au_roc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "pQF07Kf9nxl_"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    }
  ]
}
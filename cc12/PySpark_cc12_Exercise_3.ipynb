{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RXSCyhMI9l-R",
        "vtMTqCvm8_lK",
        "lO1yINjU8_lL",
        "MMNoeekI8_lL",
        "lJ-ciKuz8_lM",
        "e60PVGOD8_lM",
        "6DbB_A0W8_lN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmZxuXfB8_lE"
      },
      "source": [
        "# PySpark Exercise 3\n",
        "\n",
        "Author: ISTD, SUTD\n",
        "\n",
        "Title: Lab 12 Spark Part 2\n",
        "\n",
        "Date: March 5, 2025\n",
        "\n",
        "\n",
        "\n",
        "In this exercise, we are going to implement the KMeans clustering algorithm using Spark RDD.\n",
        "\n",
        "For parts marked with **[CODE CHANGE REQUIRED]** you need to modify or complete the code before execution.\n",
        "For parts without **[CODE CHANGE REQUIRED]** , you can just run the given code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing PySpark in Google Colab\n",
        "\n",
        "To install PySpark in Google Collab, execute the below cell. This will download Spark and install all necessary libraries for this lab."
      ],
      "metadata": {
        "id": "RXSCyhMI9l-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n"
      ],
      "metadata": {
        "id": "oE412bBe9mKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the data"
      ],
      "metadata": {
        "id": "sld-E4E9G7hM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first clone the github repo for this class:\n"
      ],
      "metadata": {
        "id": "u1dMDE9PCyHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sutd50043/cohortclass/"
      ],
      "metadata": {
        "id": "hgNE-NlBCyO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49M6XAZR8_lH"
      },
      "source": [
        "## KMeans clustering algorithm\n",
        "\n",
        "KMeans clustering algorithm is an unsupervised machine learning algorithm which groups data points into *cluster*s (or groups) based on their similarity, e.g. product their likes, movies their likes, brands they follow, TV programs and movies they watched, university and colleages they attended.\n",
        "\n",
        "Assume all the attributes of the subjects in the analysis can be represented using some scalar values, we can conduct the analysis in the following steps.\n",
        "\n",
        "1. The user/programmer specifies how many clusters he/she would like to group all the data points under. Let's say it is `K`\n",
        "2. Randomly generate `K` data points, we call them *centroids*, `c1, c2, ..., cK`.\n",
        "3. For each data point `p`, we compute the distances between `p` and `c1`, `p` and `c2`, ... Find the centroid `ci`, to which `p` is closest, we conclude `p` is in cluster `i`.\n",
        "4. For each cluster `i`, we retrieve all the data points falling in this cluster, and compute the mean. The mean will be new centroid for cluster `i`, say `ci'`\n",
        "5. Compare `c1` with `c1'`, `c2` with `c2'`, ..., `cK` with `cK'`. If all of them remains unchanged (or the differnce is lower than a threshold), we are done. Otherwise, update `c1 = c1'`, ..., `cK = cK` and go back to step 3.\n",
        "\n",
        "\n",
        "Point to note, clusters might be disappear, i.e. some centroid has zero data point inside.\n",
        "\n",
        "![](https://i.stack.imgur.com/ibYKU.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2DuGkei8_lH"
      },
      "source": [
        "Instead of using real data, we use a python script to generate the data. The python script is already written for you. It can be found in the git-cloned repo `/cohortclass/cc12/data/ex3/data/generate.py`\n",
        "\n",
        "```python\n",
        "import sys\n",
        "import random\n",
        "def gen(num_of_records, filename):\n",
        "    with open(filename,'w') as f:\n",
        "        for i in range(0,int(num_of_records)):\n",
        "            x = random.uniform(-100,100)\n",
        "            y = random.uniform(-100,100)\n",
        "            f.write(\"%.2f\\t%.2f\\n\" % (x,y))\n",
        "    f.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) > 2:\n",
        "        sys.exit(gen(sys.argv[1],sys.argv[2]))\n",
        "    else:\n",
        "        print(\"USAGE: python3 generate.py <number_of_records> <file_name>\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67oNYYIu8_lI"
      },
      "source": [
        "Let's execute the above script to create our data:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "yvtfKLfU8_lI"
      },
      "outputs": [],
      "source": [
        "!python3 cohortclass/cc12/data/ex3/generate.py 1000 points.tsv\n",
        "!python3 cohortclass/cc12/data/ex3/generate.py 10 centroids.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPkgUt3l8_lJ"
      },
      "source": [
        "## Exercise 3.1\n",
        "\n",
        "Just a reminder that if we would be working with our cluster, we would be moving the data onto the cluster using commands like this:\n",
        "\n",
        "```\n",
        "hdfs dfs -rm -r hdfs://$namenode:9000/lab12/ex3/\n",
        "hdfs dfs -mkdir -p hdfs://$namenode:9000/lab12/ex3/input/points\n",
        "hdfs dfs -put points.tsv hdfs://$namenode:9000/lab12/ex3/input/points/\n",
        "hdfs dfs -mkdir -p hdfs://$namenode:9000/lab12/ex3/input/centroids/\n",
        "hdfs dfs -put centroids.tsv hdfs://$namenode:9000/lab12/ex3/input/centroids/\n",
        "```\n",
        "\n",
        "However, we are working locally, so we do not need to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-74OQTcS8_lK"
      },
      "source": [
        "\n",
        "\n",
        "We load the spark session to create the spark context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "7QSUot-P8_lK"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "sparkSession = SparkSession.builder.appName(\"KMeans notebook\").getOrCreate()\n",
        "sc = sparkSession.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so8TyP7G8_lK"
      },
      "source": [
        "## Exercise 3.2\n",
        "\n",
        " **[CODE CHANGE REQUIRED]**\n",
        "\n",
        "Complete the definition of the following function `load_points_into_rdd`, which load the TSV data into an RDD given the path (e.g. `/content/points.tsv`).\n",
        "\n",
        "Each item in the RDD is a tuple of two float values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "g4T8KBbT8_lK"
      },
      "outputs": [],
      "source": [
        "def load_points_into_rdd(path): # TODO\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeMK6v5x8_lL"
      },
      "source": [
        "## Test case 3.2\n",
        "\n",
        "Run the following you should see, (the actual numeric values might differ, but the structure should be the same)\n",
        "```python\n",
        "[(61.4, -33.87), (19.57, -20.85), (22.95, -49.32), (42.81, 29.71), (-65.89, -75.57), (13.48, 71.92), (-17.28, -21.7), (1.79, 43.8), (11.58, -32.18), (1.73, -54.43)]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "E9HX7oMr8_lL"
      },
      "outputs": [],
      "source": [
        "# test case 1\n",
        "points = load_points_into_rdd(\"/content/points.tsv\")\n",
        "points.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtMTqCvm8_lK"
      },
      "source": [
        "### Sample answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```\n",
        "def load_points_into_rdd(namenode, path): # TODO\n",
        "    return sc.textFile(\"file://%s\" % (path)).map(lambda ln:ln.strip().split(\"\\t\")).map(lambda l:(float(l[0]), float(l[1])))\n",
        "```\n"
      ],
      "metadata": {
        "id": "eNePF3eWHPV5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFXcPNcc8_lL"
      },
      "source": [
        "\n",
        "\n",
        "## Exercise 3.3\n",
        " **[CODE CHANGE REQUIRED]**\n",
        "\n",
        "Complete the following function `euc_dist` which compute the euclidean distance between two points\n",
        "\n",
        "\n",
        "$$eucdist((x_1,y_1),(x_2,y_2)) = \\sqrt{ (x_1 - x_2)^2 + (y_1 - y_2)^2  }$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "LuQRqthJ8_lL"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def euc_dist(p1,p2):\n",
        "    '''\n",
        "    inpput\n",
        "    p1, p2: points\n",
        "    output\n",
        "    euclidean distance between p1 and p2\n",
        "    '''\n",
        "    # TODO\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVkSYrnM8_lL"
      },
      "source": [
        "## Test case 3.3\n",
        "\n",
        "Run the following you should see,\n",
        "\n",
        "```text\n",
        "1.4142135623730951\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "ewk4Rwz_8_lL"
      },
      "outputs": [],
      "source": [
        "p1 = (3.0, 1.0)\n",
        "p2 = (2.0, 2.0)\n",
        "\n",
        "euc_dist(p1,p2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO1yINjU8_lL"
      },
      "source": [
        "### Sample answer\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "import math\n",
        "def euc_dist(p1,p2):\n",
        "    '''\n",
        "    inpput\n",
        "    p1, p2: points\n",
        "    output\n",
        "    euclidean distance between p1 and p2\n",
        "    '''\n",
        "    # ANSWER\n",
        "    return math.sqrt((p1[0]-p2[0])**2 + (p1[1] - p2[1])**2)\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "BkkiUTIQJ6pc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opqGtZjG8_lL"
      },
      "source": [
        "\n",
        "\n",
        "## Exercise 3.4\n",
        " **[CODE CHANGE REQUIRED]**\n",
        "\n",
        "Complete the following function which computes the mean of of an iterator/list of points.\n",
        "\n",
        "\n",
        "`mean()` functions takes a plain python iterator of points, not an RDD of points.\n",
        "The mean of two points are defined as\n",
        "\n",
        "$$\n",
        "mean((x_1,y_1),(x_2,y_2)) = (( x_1 + x_2) / 2, (y_1 + y_2) / 2)\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "bZCkVjVh8_lL"
      },
      "outputs": [],
      "source": [
        "def mean(points):\n",
        "    '''\n",
        "    input\n",
        "    an iterator of points\n",
        "    output\n",
        "    a point that is the means of all the points in the input. if the input list is empty, return None\n",
        "    '''\n",
        "    # TODO\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxnHoDR78_lM"
      },
      "source": [
        "## Test case 3\n",
        "\n",
        "Run the following you should see,\n",
        "\n",
        "```text\n",
        "(4,5)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "pyvn60uK8_lM"
      },
      "outputs": [],
      "source": [
        "points = ((x,y) for x in range(0, 10) for y in range(1,11))\n",
        "\n",
        "mean(points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMNoeekI8_lL"
      },
      "source": [
        "### Sample answer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "def mean(points):\n",
        "    '''\n",
        "    input\n",
        "    an iterator of points\n",
        "    output\n",
        "    a point that is the means of all the points in the input. if the input list is empty, return None\n",
        "    '''\n",
        "    # ANSWER\n",
        "    pts = list(points)\n",
        "    count = len(pts)\n",
        "    if count == 0:\n",
        "        return None\n",
        "    else:\n",
        "        xs = map(lambda p:p[0], pts)\n",
        "        ys = map(lambda p:p[1], pts)\n",
        "        return (sum(xs)/len(pts), sum(ys)/len(pts))\n",
        "```"
      ],
      "metadata": {
        "id": "3ndrpY8YKSc2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLnGeE538_lM"
      },
      "source": [
        "## Exercise 3.5\n",
        " **[CODE CHANGE REQUIRED]**\n",
        "Complete the following function which finds the nearest centroids for each points in the RDD.\n",
        "\n",
        "```text\n",
        "Hint: Let r1 and r2 be RDDs, r1.cartesion(r2) produces the cartesian product of r1 and r2.\n",
        "When finding the nearest centroid w.r.t to a point from a list of centroids, it is useful to think of it as a reduce operation.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "BbccdntR8_lM"
      },
      "outputs": [],
      "source": [
        "def nearest(points, centroids):\n",
        "    '''\n",
        "    inputs\n",
        "    points: an RDD of points\n",
        "    centroids: an RDD of the current centroids\n",
        "\n",
        "    output:\n",
        "    point_and_nearestcentroids : an RDD of pairs, each pair consists of a point and the nearest centroid it belongs to\n",
        "    '''\n",
        "    # TODO\n",
        "\n",
        "    point_and_nearcentroids = None\n",
        "    return point_and_nearcentroids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQr6vxEL8_lM"
      },
      "source": [
        "## Test Case 3.5\n",
        "\n",
        "Run the following you should see, (the actual numeric values might differ, but the structure should be the same)\n",
        "\n",
        "\n",
        "```text\n",
        "[((31.34, 39.54), (10.94, 64.76)), ((63.18, -54.98), (78.66, -91.58)), ((-96.53, 54.69), (-53.93, 76.38)), ((-89.91, 9.5), (-42.27, 24.16)), ((95.43, 57.26), (10.94, 64.76)), ((74.38, -61.52), (78.66, -91.58)), ((77.21, -69.08), (78.66, -91.58)), ((-84.04, -74.41), (11.81, -27.94)), ((7.44, 45.33), (10.94, 64.76)), ((30.63, 52.08), (10.94, 64.76))]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "4-8V2U2u8_lM"
      },
      "outputs": [],
      "source": [
        "points = load_points_into_rdd(\"/content/points.tsv\")\n",
        "centroids = load_points_into_rdd(\"/content/centroids.tsv\")\n",
        "\n",
        "nearest(points, centroids).take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ-ciKuz8_lM"
      },
      "source": [
        "### Sample Answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "def nearest(points, centroids):\n",
        "    '''\n",
        "    inputs\n",
        "    points: an RDD of points\n",
        "    centroids: an RDD of the current centroids\n",
        "    \n",
        "    output:\n",
        "    point_and_nearestcentroids : an RDD of pairs, each pair consists of a point and the nearest centroid it belongs to\n",
        "    '''\n",
        "    # ANSWER\n",
        "    point_cross_centroids = points.cartesian(centroids)\n",
        "    point_cross_centroids_distance = point_cross_centroids.map(lambda pc: (pc[0],(pc[1],euc_dist(pc[0],pc[1]))))\n",
        "    point_and_nearcentroids = point_cross_centroids_distance.reduceByKey(lambda cd1, cd2:  cd1 if cd1[1] < cd2[1] else cd2 ).map(lambda pc: (pc[0], pc[1][0]))\n",
        "    return point_and_nearcentroids\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "EVLeNW8BvpC5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyLSNuki8_lM"
      },
      "source": [
        "## Exercise 3.6\n",
        "\n",
        " **[CODE CHANGE REQUIRED]**\n",
        "\n",
        "Complete the following function which executes one iteration of the K-Means algorithm.\n",
        "\n",
        "```text\n",
        "Hint: Recall the difference between reduceByKey and groupByKey\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "90OcFK4p8_lM"
      },
      "outputs": [],
      "source": [
        "def iteration(points, centroids):\n",
        "    '''\n",
        "    inputs\n",
        "    points: an RDD of points\n",
        "    centroids: an RDD of the current centroids\n",
        "\n",
        "    output\n",
        "    current_and_new_centroids: an RDD of pairs, each pair consists of a current centroid and the new centroid\n",
        "    '''\n",
        "\n",
        "    # Step a: for each point, compute the euclidean_dinstance with each centroid, find the nearest centroid\n",
        "    point_and_nearcentroids = nearest(points, centroids)\n",
        "    # Step b: flip the pairs from step b, and create an RDD of nearest centroid and points (note: the nearest centroids are still the current centroids)\n",
        "    # TODO\n",
        "    nearcentroid_and_points = None\n",
        "    # Step c: compute the new centroids\n",
        "    # TODO\n",
        "    current_and_newcentroids = None\n",
        "    return current_and_newcentroids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmbD6FAt8_lM"
      },
      "source": [
        "## Test case 3.6\n",
        "\n",
        "Run the following you should see, (the actual numeric values might differ, but the structure should be the same)\n",
        "\n",
        "```text\n",
        "[((-42.27, 24.16), (-63.50617346938779, -4.715561224489797)), ((34.19, -5.47), (65.24037313432835, 4.253208955223882)), ((73.78, -94.16), (43.22766666666668, -84.53166666666665)), ((78.66, -91.58), (83.09037735849053, -62.727924528301884)), ((11.81, -27.94), (-18.85223809523809, -62.081285714285706)), ((-55.65, 97.37), (-42.820499999999996, 94.566)), ((11.1, 2.86), (8.047450980392156, 10.851764705882351)), ((10.94, 64.76), (38.894331550802164, 69.78598930481279)), ((-68.4, 93.57), (-83.12192307692308, 83.44)), ((-53.93, 76.38), (-56.9511111111111, 63.98825396825399))]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "2TeJbWbt8_lN"
      },
      "outputs": [],
      "source": [
        "points = load_points_into_rdd(\"/content/points.tsv\")\n",
        "centroids = load_points_into_rdd(\"/content/centroids.tsv\")\n",
        "r = iteration(points,centroids)\n",
        "r.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e60PVGOD8_lM"
      },
      "source": [
        "### Sample answer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "def iteration(points, centroids):\n",
        "    '''\n",
        "    inputs\n",
        "    points: an RDD of points\n",
        "    centroids: an RDD of the current centroids\n",
        "    \n",
        "    output\n",
        "    current_and_new_centroids: an RDD of pairs, each pair consists of a current centroid and the new centroid\n",
        "    '''\n",
        "    # Step a: for each point, compute the euclidean_dinstance with each centroid, find the nearest centroid\n",
        "    point_and_nearcentroids = nearest(points, centroids)\n",
        "    # Step b: flip the pairs from step b, and create an RDD of nearest centroid and points (note: the nearest centroids are still the current centroids)\n",
        "    nearcentroid_and_points = point_and_nearcentroids.map(lambda p: (p[1], p[0]))\n",
        "    # Step c: compute the new centroids\n",
        "    current_and_newcentroids = nearcentroid_and_points.groupByKey().map(lambda kvs: (kvs[0], mean(kvs[1])))\n",
        "    return current_and_newcentroids\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "uIXKb30vLY2p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KV2MF2a8_lN"
      },
      "source": [
        "\n",
        "\n",
        "## KMeans\n",
        "\n",
        "To define KMeans, we just need two more actions,\n",
        "\n",
        "1. `forAll(rdd,p)` which checks whether all elements in `rdd` satisfy the predicate `p`.\n",
        "2. `no_change(no_change(centroid_and_newcentroids,tolerance)` which applies a conditional check to all pairs of current and new centroids. It yields True if none of the new centroids is `None`, and the euclidean distances between the currents and new centroids are less than the tolerance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKwLI1d38_lN"
      },
      "source": [
        "### Exercise 3.7\n",
        "\n",
        "Complete the `forAll` function. You don't need to change `no_change` function.\n",
        "\n",
        " **[CODE CHANGE REQUIRED]**\n",
        "\n",
        "Complete the following function which executes one iteration of the K-Means algorithm.\n",
        "\n",
        "```text\n",
        "Hint: You can implement it using\n",
        "1. the aggregate function or\n",
        "2. map and reduce functions.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "sB_zsImA8_lN"
      },
      "outputs": [],
      "source": [
        "def forAll(rdd, p):\n",
        "    '''\n",
        "    input:\n",
        "    rdd : an RDD\n",
        "    p : a predicate, a lambda function that takes a value and return a boolean. p will be applied to all elements in rdd\n",
        "\n",
        "    output:\n",
        "    True or False\n",
        "    '''\n",
        "    # TODO:\n",
        "\n",
        "    # return False\n",
        "\n",
        "\n",
        "def no_change(centroid_and_newcentroids,tolerance):\n",
        "    return forAll(centroid_and_newcentroids, lambda p:  p[1] is not None and euc_dist(p[0], p[1]) < tolerance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTurs3IP8_lN"
      },
      "source": [
        "### Test Case 3.7\n",
        "\n",
        "Run the following, we should see `True`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "1vewBFcU8_lS"
      },
      "outputs": [],
      "source": [
        "test_vs = sc.parallelize([2,4,0,6])\n",
        "forAll(test_vs ,lambda x:x % 2 == 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DbB_A0W8_lN"
      },
      "source": [
        "### Sample answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```python\n",
        "def forAll(rdd, p):\n",
        "    '''\n",
        "    input:\n",
        "    rdd : an RDD\n",
        "    p : a predicate, a lambda function that takes a value and return a boolean. p will be applied to all elements in rdd\n",
        "    \n",
        "    output:\n",
        "    True or False\n",
        "    '''\n",
        "    # ANSWER\n",
        "    return rdd.map(p).reduce(lambda x,y: x and y)\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "Jg8jSZjyLiS5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TIjG6j58_lS"
      },
      "source": [
        "### Lastly\n",
        "Lastly, the `kmeans` function is defined by a for-loop in which\n",
        "\n",
        "1. we call `iteration` to compute the new centroids,\n",
        "2. then check whether there is any change between the current and new centroids via `no_change`.\n",
        "    2.1. If there is changes, it goes back to the loop by sending the new centroids to be current centroids,\n",
        "    2.2. otherwise it exits the loop and compute the membership between the points and the lastest centroids.\n",
        "\n",
        "Note the use of `.persist()`. try to re-run it again by commenting away the statements using `.persist()`, it will take a longer time to converge.\n",
        "\n",
        "The code is written for you, you don't need to change anything unless you want to experiement with `.persist()` and without.\n",
        "\n",
        "Note: K-means is a computationally expensive algorithm. On Colab we only have 2 CPUs at our disposal, so this may take a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "83Owz6c_8_lS"
      },
      "outputs": [],
      "source": [
        "def kmeans(points, centroids, num_iters, tolerance=2):\n",
        "    points.persist()\n",
        "    for i in range(0, num_iters):\n",
        "        centroid_and_newcentroids = iteration(points,centroids)\n",
        "        if no_change(centroid_and_newcentroids,tolerance):\n",
        "            break;\n",
        "        centroids = centroid_and_newcentroids.map(lambda p:p[1]).filter(lambda c: c is not None)\n",
        "        centroids.persist()\n",
        "        # print(i,centroids.collect())\n",
        "\n",
        "    return nearest(points, centroids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "8I_A88pZ8_lS"
      },
      "outputs": [],
      "source": [
        "points = load_points_into_rdd(\"/content/points.tsv\")\n",
        "centroids = load_points_into_rdd(\"/content/centroids.tsv\")\n",
        "\n",
        "clusters = kmeans(points, centroids, 100, 2)\n",
        "\n",
        "clusters.take(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "9ZJ5Xpw38_lS"
      },
      "outputs": [],
      "source": [
        "sc.stop()"
      ]
    }
  ]
}